#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Anis Linguistic Radar - Web Version (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©)
Flask-based web application for Arabic text analysis
Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø­ÙÙˆØ¸Ø© Ù„Ù„Ù…Ø·ÙˆØ± Ø£Ù†ÙŠØ³ ÙÙŠÙ„Ø§Ù„ÙŠ
"""

from flask import Flask, render_template, request, jsonify
import matplotlib
matplotlib.use('Agg')  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ù„ÙÙŠØ© ØºÙŠØ± Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ© (Ù„Ù„Ø³ÙŠØ±ÙØ±)
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
import os
import csv
import hashlib
import logging
import uuid
from collections import Counter, OrderedDict
from datetime import datetime
import arabic_reshaper
from bidi.algorithm import get_display
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ wordcloud (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
try:
    from wordcloud import WordCloud
    WORDCLOUD_AVAILABLE = True
except ImportError:
    WORDCLOUD_AVAILABLE = False
    WordCloud = None

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ camel-tools (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
try:
    from camel_tools.sentiment import SentimentAnalyzer
    CAMEL_AVAILABLE = True
except ImportError:
    CAMEL_AVAILABLE = False
    SentimentAnalyzer = None

# ---------------------------- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø«Ø§Ø¨ØªØ© ----------------------------
app = Flask(__name__)
app.config['SECRET_KEY'] = 'anis-secret-key-change-in-production'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB Ø­Ø¯ Ù„Ù„Ù…Ù„ÙØ§Øª

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
STATIC_IMAGES_DIR = os.path.join(BASE_DIR, 'static', 'images')
HISTORY_FILE = os.path.join(BASE_DIR, 'history.csv')
FONT_PATH = os.path.join(BASE_DIR, 'Amiri-Regular.ttf')

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØµÙˆØ± Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
os.makedirs(STATIC_IMAGES_DIR, exist_ok=True)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù…Ø®Ø·Ø·Ø§Øª matplotlib
font_prop = None
if os.path.exists(FONT_PATH):
    try:
        font_prop = fm.FontProperties(fname=FONT_PATH)
        logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø®Ø· Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù…Ø®Ø·Ø·Ø§Øª Matplotlib")
    except Exception as e:
        logging.warning(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø®Ø· Ù„Ù…Ø®Ø·Ø·Ø§Øª Matplotlib: {e}")
else:
    logging.warning("âš ï¸ Ø®Ø· Amiri ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ù‚Ø¯ Ù„Ø§ ØªØ¸Ù‡Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ ÙÙŠ Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª.")

# Ø«ÙˆØ§Ø¨Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
MIN_TEXT_LENGTH = 10
MAX_TEXT_LENGTH = 20000
RADAR_CATEGORIES = [
    "Ø§Ù„Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§",
    "Ø§Ù„ØªÙˆØ§Ø²Ù† Ø§Ù„ØµÙˆØªÙŠ",
    "Ø§Ù„Ø¬Ù‡Ø±",
    "Ø§Ù„Ù‡Ù…Ø³",
    "Ø·ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©",
    "Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª"
]

# Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
VOICED = set("Ø¨Ø¬ØªØ¯Ø°Ø±Ø²Ø¶Ø¸Ø¹ØºÙ‚Ù„Ù…Ù†ÙˆÙŠ")
VOICELESS = set("Ø­Ø«Ø³ØµØ´ÙÙƒÙ‡Øª")
PUNCTUATIONS = set(".,;:!?ØŸØŒØ›")

# ---------------------------- Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© ----------------------------
def reshape_arabic(text):
    """Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø¹Ø±Ø¶ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­"""
    if not text:
        return text
    try:
        reshaped = arabic_reshaper.reshape(text)
        return get_display(reshaped)
    except Exception as e:
        logging.error(f"Ø®Ø·Ø£ ÙÙŠ reshape: {e}")
        return text

# ---------------------------- Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª (Cache) ----------------------------
class AnalysisCache:
    """ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… hash Ø§Ù„Ù†Øµ ÙƒÙ…ÙØªØ§Ø­"""
    def __init__(self, max_size=50):
        self.cache = OrderedDict()
        self.max_size = max_size

    def _hash(self, text):
        return hashlib.sha256(text.encode('utf-8')).hexdigest()

    def get(self, text):
        key = self._hash(text)
        if key in self.cache:
            self.cache.move_to_end(key)
            logging.info("âœ… ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„ÙƒØ§Ø´")
            return self.cache[key]
        return None

    def put(self, text, result):
        key = self._hash(text)
        if key in self.cache:
            self.cache.move_to_end(key)
        else:
            if len(self.cache) >= self.max_size:
                self.cache.popitem(last=False)
            self.cache[key] = result
        logging.info("ğŸ“¦ ØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„ÙƒØ§Ø´")

cache = AnalysisCache()

# ---------------------------- Ù…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© ----------------------------
class FeatureExtractor:
    def extract(self, text):
        if not text or len(text.strip()) == 0:
            return [0.0] * 8

        words = text.split()
        total_words = len(words)
        total_chars = len(text)

        # 1. Ø§Ù„Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§
        char_counts = Counter(text)
        entropy = -sum((count/total_chars) * np.log2(count/total_chars) for count in char_counts.values()) if total_chars else 0

        # 2. Ø§Ù„ØªÙˆØ§Ø²Ù† Ø§Ù„ØµÙˆØªÙŠ
        voiced = sum(1 for c in text if c in VOICED)
        voiceless = sum(1 for c in text if c in VOICELESS)
        total_phonemes = voiced + voiceless
        if total_phonemes:
            voiced_pct = voiced / total_phonemes * 100
            voiceless_pct = voiceless / total_phonemes * 100
            balance = 1 - abs(voiced_pct - voiceless_pct) / 100
        else:
            voiced_pct = voiceless_pct = 50.0
            balance = 1.0

        # 3. Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©
        avg_word = np.mean([len(w) for w in words]) if words else 0.0

        # 4. Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø©
        sentences = [s.strip() for s in text.replace('!', '.').replace('ØŸ', '.').replace('ØŒ', '.').split('.') if s.strip()]
        avg_sentence = total_words / len(sentences) if sentences else total_words

        # 5. Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
        unique_words = len(set(words))
        richness = unique_words / total_words if total_words else 0.0

        # 6. Ù†Ø³Ø¨Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
        punct_count = sum(1 for c in text if c in PUNCTUATIONS)
        punct_ratio = punct_count / total_chars if total_chars else 0.0

        return [
            entropy,
            balance,
            voiced_pct,
            voiceless_pct,
            avg_word,
            avg_sentence,
            punct_ratio * 100,
            richness * 100
        ]

    def advanced_stylometry(self, text):
        words = text.split()
        total = len(words)
        unique = len(set(words))
        ttr = unique / total if total else 0.0

        freq = Counter(words)
        hapax = sum(1 for w in freq if freq[w] == 1)
        hapax_ratio = hapax / total if total else 0.0

        content_words = [w for w in words if len(w) > 3]
        lexical_density = len(content_words) / total if total else 0.0

        return ttr, hapax_ratio, lexical_density

feature_extractor = FeatureExtractor()

# ---------------------------- Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ----------------------------
class DeepSentimentAnalyzer:
    def __init__(self):
        self.analyzer = None
        if CAMEL_AVAILABLE:
            try:
                self.analyzer = SentimentAnalyzer.pretrained()
                logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† camel-tools")
            except Exception as e:
                logging.warning(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ camel-tools: {e}")
        else:
            logging.info("âš ï¸ camel-tools ØºÙŠØ± Ù…Ø«Ø¨ØªØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø¨Ø³ÙŠØ·")

    def analyze(self, text):
        if self.analyzer is not None:
            try:
                result = self.analyzer.predict([text[:512]])[0]
                confidence = 0.85  # ØªÙ‚Ø¯ÙŠØ±ÙŠ
                emotions = self._extract_emotions(text)
                return result, confidence, emotions
            except Exception as e:
                logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚: {e}")

        # Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø¨Ø³ÙŠØ·
        pos_words = {'Ø­Ø¨', 'Ø³Ø¹ÙŠØ¯', 'ÙØ±Ø­', 'Ø¬Ù…ÙŠÙ„', 'Ø±Ø§Ø¦Ø¹', 'Ù…Ù…ØªØ§Ø²', 'ÙŠØ¨ØªØ³Ù…', 'Ø£Ù…Ù„', 'ØªÙØ§Ø¤Ù„', 'Ù†ÙˆØ±',
                     'Ø¨Ù‡Ø¬Ø©', 'Ø³Ø±ÙˆØ±', 'Ù„Ø·ÙŠÙ', 'Ø¹Ø¸ÙŠÙ…', 'Ù…Ø¨Ø¯Ø¹', 'Ù†Ø§Ø¬Ø­', 'Ù…Ø´Ø±Ù‚'}
        neg_words = {'Ø­Ø²Ù†', 'Ø¨ÙƒØ§Ø¡', 'Ø£Ù„Ù…', 'ÙƒØ¦ÙŠØ¨', 'Ù…Ø¤Ù„Ù…', 'Ø³ÙŠØ¡', 'Ù‚Ø¨ÙŠØ­', 'Ø¸Ù„Ø§Ù…', 'Ø®ÙˆÙ', 'ÙØ²Ø¹',
                     'ØµØ¹Ø¨', 'Ø¹Ø³ÙŠØ±', 'Ù…Ø²Ø¹Ø¬', 'ØºØ¶Ø¨', 'ÙƒØ±Ø§Ù‡ÙŠØ©', 'Ø­Ù‚Ø¯', 'Ø¶ÙŠÙ‚', 'Ù‡Ù…', 'ÙƒØ§Ø±Ø«Ø©'}

        words = text.split()
        pos_count = sum(1 for w in words if w in pos_words)
        neg_count = sum(1 for w in words if w in neg_words)
        total = pos_count + neg_count

        if total == 0:
            return "Ù…Ø­Ø§ÙŠØ¯", 0.5, {}
        pos_ratio = pos_count / total
        neg_ratio = neg_count / total
        if pos_ratio > 0.66:
            sentiment = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
        elif neg_ratio > 0.66:
            sentiment = "Ø³Ù„Ø¨ÙŠ"
        else:
            sentiment = "Ù…Ø­Ø§ÙŠØ¯"
        confidence = max(pos_ratio, neg_ratio)
        emotions = {'positive': pos_count, 'negative': neg_count}
        return sentiment, confidence, emotions

    def _extract_emotions(self, text):
        emotions = {'ÙØ±Ø­': 0, 'Ø­Ø²Ù†': 0, 'ØºØ¶Ø¨': 0, 'Ù…ÙØ§Ø¬Ø£Ø©': 0, 'Ø®ÙˆÙ': 0}
        joy_words = ['Ø³Ø¹ÙŠØ¯', 'ÙØ±Ø­', 'Ù…Ø¨Ø³ÙˆØ·', 'ÙŠØ¨ØªØ³Ù…', 'Ø¬Ù…ÙŠÙ„', 'Ø±Ø§Ø¦Ø¹']
        sad_words = ['Ø­Ø²ÙŠÙ†', 'Ø¨ÙƒØ§Ø¡', 'Ø£Ù„Ù…', 'ÙƒØ¦ÙŠØ¨', 'Ù…Ø¤Ù„Ù…']
        anger_words = ['ØºØ§Ø¶Ø¨', 'ØºØ¶Ø¨', 'ÙƒØ±Ù‡', 'Ø­Ù‚Ø¯', 'Ù…Ø²Ø¹Ø¬']
        surprise_words = ['Ù…ÙØ§Ø¬Ø£Ø©', 'Ù…Ø°Ù‡Ù„', 'Ø¹Ø¬ÙŠØ¨', 'ØºØ±ÙŠØ¨']
        fear_words = ['Ø®Ø§Ø¦Ù', 'Ø®ÙˆÙ', 'ÙØ²Ø¹', 'Ù…Ø±Ø¹ÙˆØ¨']
        words = text.split()
        for word in words:
            if word in joy_words:
                emotions['ÙØ±Ø­'] += 1
            elif word in sad_words:
                emotions['Ø­Ø²Ù†'] += 1
            elif word in anger_words:
                emotions['ØºØ¶Ø¨'] += 1
            elif word in surprise_words:
                emotions['Ù…ÙØ§Ø¬Ø£Ø©'] += 1
            elif word in fear_words:
                emotions['Ø®ÙˆÙ'] += 1
        return emotions

sentiment_analyzer = DeepSentimentAnalyzer()

# ---------------------------- Ø¯ÙˆØ§Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© ----------------------------
def create_radar_chart(stats, filename, dark_mode=True):
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø· Ø§Ù„Ø±Ø§Ø¯Ø§Ø± ÙˆØ­ÙØ¸Ù‡ ÙÙŠ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø¯"""
    bg = "#0B0F19" if dark_mode else "#f0f0f0"
    text_color = "#F7FAFC" if dark_mode else "#333333"
    try:
        values = [
            stats[0] * 10,
            stats[1] * 100,
            stats[2],
            stats[3],
            stats[4] * 2,
            stats[7]
        ]
        values += values[:1]
        angles = np.linspace(0, 2*np.pi, len(RADAR_CATEGORIES), endpoint=False).tolist()
        angles += angles[:1]

        cat_reshaped = [reshape_arabic(cat) for cat in RADAR_CATEGORIES]

        fig = plt.figure(figsize=(6, 5), dpi=80, facecolor=bg)
        ax = fig.add_subplot(111, polar=True)
        ax.set_facecolor("#1a1f2e" if dark_mode else "#e0e0e0")
        ax.plot(angles, values, color="#D4AF37", linewidth=3, marker='o')
        ax.fill(angles, values, color="#D4AF37", alpha=0.3)
        ax.set_yticklabels([])
        ax.set_xticks(angles[:-1])
        if font_prop:
            ax.set_xticklabels(cat_reshaped, fontproperties=font_prop, color=text_color, size=10)
        else:
            ax.set_xticklabels(cat_reshaped, color=text_color, size=10)
        fig.savefig(filename, bbox_inches='tight', facecolor=bg)
        plt.close(fig)
        return True
    except Exception as e:
        logging.error(f"ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø§Ø¯Ø§Ø±: {e}")
        return False

def create_bar_chart(text, filename, dark_mode=True):
    """Ù…Ø®Ø·Ø· ØªÙˆØ²ÙŠØ¹ Ø£Ø·ÙˆØ§Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª"""
    bg = "#0B0F19" if dark_mode else "#f0f0f0"
    text_color = "#F7FAFC" if dark_mode else "#333333"
    try:
        words = text.split()
        word_lengths = [len(w) for w in words if w]
        bins = range(1, 12)
        hist, _ = np.histogram(word_lengths, bins=bins)
        labels = [f"{i}-{i+1}" for i in range(1, 11)]

        fig = plt.figure(figsize=(6,5), dpi=80, facecolor=bg)
        ax = fig.add_subplot(111)
        ax.set_facecolor("#1a1f2e" if dark_mode else "#e0e0e0")
        ax.bar(labels, hist, color="#4FD1C5", edgecolor="#D4AF37", linewidth=1.5)

        xlabel = reshape_arabic("Ø·ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø© (Ø­Ø±ÙˆÙ)")
        ylabel = reshape_arabic("Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª")
        title = reshape_arabic("ØªÙˆØ²ÙŠØ¹ Ø£Ø·ÙˆØ§Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª")

        if font_prop:
            ax.set_xlabel(xlabel, fontproperties=font_prop, color=text_color)
            ax.set_ylabel(ylabel, fontproperties=font_prop, color=text_color)
            ax.set_title(title, fontproperties=font_prop, color="#D4AF37")
            for label in ax.get_xticklabels():
                label.set_fontproperties(font_prop)
            for label in ax.get_yticklabels():
                label.set_fontproperties(font_prop)
        else:
            ax.set_xlabel(xlabel, color=text_color)
            ax.set_ylabel(ylabel, color=text_color)
            ax.set_title(title, color="#D4AF37")

        ax.tick_params(colors=text_color)
        for spine in ax.spines.values():
            spine.set_color("#D4AF37")
        fig.savefig(filename, bbox_inches='tight', facecolor=bg)
        plt.close(fig)
        return True
    except Exception as e:
        logging.error(f"ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ø´Ø±ÙŠØ·ÙŠ: {e}")
        return False

def create_wordcloud(text, filename, dark_mode=True):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª"""
    bg = "#0B0F19" if dark_mode else "#f0f0f0"
    if not WORDCLOUD_AVAILABLE:
        # Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø®Ø·Ø£ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¥Ø±Ø¬Ø§Ø¹ Ø®Ø·Ø£
        fig = plt.figure(figsize=(6,5), facecolor=bg)
        ax = fig.add_subplot(111)
        ax.text(0.5,0.5, "Ù…ÙƒØªØ¨Ø© wordcloud ØºÙŠØ± Ù…Ø«Ø¨ØªØ©", color='red', ha='center')
        fig.savefig(filename, bbox_inches='tight', facecolor=bg)
        plt.close(fig)
        return True

    try:
        reshaped = reshape_arabic(text)
        # Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙŠ wordcloud
        if any('\u0600' <= c <= '\u06FF' for c in reshaped):
            processed = reshaped[::-1]  # Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù†Øµ
        else:
            processed = reshaped

        wc = WordCloud(
            width=500, height=400,
            background_color='#0D1117' if dark_mode else '#f0f0f0',
            font_path=FONT_PATH if os.path.exists(FONT_PATH) else None,
            colormap='viridis',
            random_state=42
        ).generate(processed)

        fig = plt.figure(figsize=(6,5), dpi=80, facecolor=bg)
        ax = fig.add_subplot(111)
        ax.imshow(wc, interpolation='bilinear')
        ax.axis("off")
        title = reshape_arabic("Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª")
        if font_prop:
            ax.set_title(title, fontproperties=font_prop, color="#D4AF37", fontsize=14)
        else:
            ax.set_title(title, color="#D4AF37", fontsize=14)
        fig.savefig(filename, bbox_inches='tight', facecolor=bg)
        plt.close(fig)
        return True
    except Exception as e:
        logging.error(f"ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø­Ø§Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {e}")
        fig = plt.figure(figsize=(6,5), facecolor=bg)
        ax = fig.add_subplot(111)
        ax.text(0.5,0.5, f"Ø®Ø·Ø£: {e}", color='red', ha='center')
        fig.savefig(filename, bbox_inches='tight', facecolor=bg)
        plt.close(fig)
        return True

# ---------------------------- Ø¯ÙˆØ§Ù„ CSV ----------------------------
def log_to_csv(text, sentiment, stats):
    """ØªØ³Ø¬ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ù…Ù„Ù CSV"""
    try:
        file_exists = os.path.isfile(HISTORY_FILE)
        with open(HISTORY_FILE, 'a', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(['Ø§Ù„Ù†Øµ (Ù…Ø®ØªØµØ±)', 'Ø§Ù„Ù…Ø´Ø§Ø¹Ø±', 'Ø§Ù„Ø¥Ù†ØªØ±ÙˆØ¨ÙŠØ§', 'Ø§Ù„ØªØ§Ø±ÙŠØ®'])
            writer.writerow([
                text[:50] + "...",
                sentiment,
                f"{stats[0]:.2f}",
                datetime.now().strftime("%Y-%m-%d %H:%M")
            ])
    except Exception as e:
        logging.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")

def read_history(limit=50):
    """Ù‚Ø±Ø§Ø¡Ø© Ø¢Ø®Ø± limit Ø³Ø¬Ù„ Ù…Ù† Ù…Ù„Ù CSV"""
    if not os.path.exists(HISTORY_FILE):
        return []
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.reader(f)
            rows = list(reader)
        if len(rows) <= 1:
            return []
        # Ù†Ø±ÙŠØ¯ Ø¢Ø®Ø± limit ØµÙ (Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†)
        return rows[1:][-limit:]
    except Exception as e:
        logging.error(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø³Ø¬Ù„: {e}")
        return []

# ---------------------------- Ù†Ù‚Ø§Ø· Ù†Ù‡Ø§ÙŠØ© Flask ----------------------------

@app.route('/')
def index():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return render_template('index.html')

@app.route('/analyze', methods=['POST'])
def analyze():
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø³Ù„"""
    data = request.get_json()
    if not data or 'text' not in data:
        return jsonify({'error': 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ'}), 400

    text = data['text'].strip()
    if len(text) < MIN_TEXT_LENGTH:
        return jsonify({'error': f'Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ (Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ {MIN_TEXT_LENGTH} Ø­Ø±Ù)'}), 400
    if len(text) > MAX_TEXT_LENGTH:
        text = text[:MAX_TEXT_LENGTH]  # Ø§Ù‚ØªØ·Ø§Ø¹

    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒØ§Ø´
    cached = cache.get(text)
    if cached:
        stats, sentiment, confidence, emotions = cached
        from_cache = True
    else:
        stats = feature_extractor.extract(text)
        sentiment, confidence, emotions = sentiment_analyzer.analyze(text)
        cache.put(text, (stats, sentiment, confidence, emotions))
        from_cache = False

    # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø³Ù…Ø§Ø¡ Ù…Ù„ÙØ§Øª ÙØ±ÙŠØ¯Ø© Ù„Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
    plot_id = str(uuid.uuid4())
    radar_filename = os.path.join(STATIC_IMAGES_DIR, f'radar_{plot_id}.png')
    bar_filename = os.path.join(STATIC_IMAGES_DIR, f'bar_{plot_id}.png')
    wc_filename = os.path.join(STATIC_IMAGES_DIR, f'wc_{plot_id}.png')

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³ÙˆÙ…
    dark_mode = data.get('dark_mode', True)  # ÙŠÙ…ÙƒÙ† ØªÙ…Ø±ÙŠØ±Ù‡Ø§ Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
    radar_ok = create_radar_chart(stats, radar_filename, dark_mode)
    bar_ok = create_bar_chart(text, bar_filename, dark_mode)
    wc_ok = create_wordcloud(text, wc_filename, dark_mode)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    try:
        vectorizer = TfidfVectorizer(max_features=5)
        sentences = text.split('.')
        if len(sentences) < 2:
            sentences = [text]
        tfidf = vectorizer.fit_transform(sentences)
        keywords = vectorizer.get_feature_names_out().tolist()
    except Exception as e:
        keywords = ["ØºÙŠØ± Ù…ØªØ§Ø­"]

    # Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø³Ù„ÙˆØ¨ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©
    ttr, hapax, lex = feature_extractor.advanced_stylometry(text)

    # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ CSV
    log_to_csv(text, sentiment, stats)

    # ØªØ¬Ù‡ÙŠØ² Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØ± (Ù†Ø³Ø¨ÙŠØ© Ù„Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø«Ø§Ø¨Øª)
    base_url = '/static/images/'
    response = {
        'success': True,
        'from_cache': from_cache,
        'text_preview': text[:200] + '...' if len(text) > 200 else text,
        'sentiment': sentiment,
        'confidence': confidence,
        'emotions': emotions,
        'stats': {
            'entropy': round(stats[0], 2),
            'balance': round(stats[1]*100, 1),
            'voiced': round(stats[2], 1),
            'voiceless': round(stats[3], 1),
            'avg_word': round(stats[4], 2),
            'avg_sentence': round(stats[5], 2),
            'punct_ratio': round(stats[6], 1),
            'richness': round(stats[7], 1)
        },
        'advanced': {
            'ttr': round(ttr*100, 1),
            'hapax': round(hapax*100, 1),
            'lexical_density': round(lex*100, 1)
        },
        'keywords': keywords[:5],
        'plots': {
            'radar': base_url + f'radar_{plot_id}.png' if radar_ok else None,
            'bar': base_url + f'bar_{plot_id}.png' if bar_ok else None,
            'wordcloud': base_url + f'wc_{plot_id}.png' if wc_ok else None
        }
    }
    return jsonify(response)

@app.route('/compare', methods=['POST'])
def compare():
    """Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ù†ØµÙŠÙ†"""
    data = request.get_json()
    text1 = data.get('text1', '').strip()
    text2 = data.get('text2', '').strip()
    if not text1 or not text2:
        return jsonify({'error': 'Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ ÙƒÙ„Ø§ Ø§Ù„Ù†ØµÙŠÙ†'}), 400

    try:
        vectorizer = TfidfVectorizer()
        tfidf = vectorizer.fit_transform([text1, text2])
        sim = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0] * 100

        stats1 = feature_extractor.extract(text1)
        stats2 = feature_extractor.extract(text2)

        result = {
            'similarity': round(sim, 2),
            'stats1': {
                'entropy': round(stats1[0], 2),
                'richness': round(stats1[7], 1)
            },
            'stats2': {
                'entropy': round(stats2[0], 2),
                'richness': round(stats2[7], 1)
            }
        }
        return jsonify(result)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/history', methods=['GET'])
def history():
    """Ø¥Ø±Ø¬Ø§Ø¹ Ø¢Ø®Ø± Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ù† CSV"""
    records = read_history(50)
    # ØªÙ†Ø³ÙŠÙ‚ Ù„Ù„Ø¹Ø±Ø¶
    history_list = []
    for row in records:
        if len(row) >= 4:
            history_list.append({
                'text': row[0],
                'sentiment': row[1],
                'entropy': row[2],
                'time': row[3]
            })
    return jsonify(history_list)

@app.route('/cleanup_images', methods=['POST'])
def cleanup_images():
    """Ø­Ø°Ù Ø§Ù„ØµÙˆØ± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø£Ùˆ Ø¨Ø´ÙƒÙ„ Ø¯ÙˆØ±ÙŠ)"""
    # Ø­Ø°Ù Ø§Ù„ØµÙˆØ± Ø§Ù„Ø£Ù‚Ø¯Ù… Ù…Ù† Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
    import time
    now = time.time()
    deleted = 0
    for fname in os.listdir(STATIC_IMAGES_DIR):
        if fname.startswith(('radar_', 'bar_', 'wc_')) and fname.endswith('.png'):
            path = os.path.join(STATIC_IMAGES_DIR, fname)
            if now - os.path.getmtime(path) > 3600:  # Ø£Ù‚Ø¯Ù… Ù…Ù† Ø³Ø§Ø¹Ø©
                os.remove(path)
                deleted += 1
    return jsonify({'deleted': deleted})

# ---------------------------- ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ----------------------------
if __name__ == '__main__':
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø§Ø¯Ù… Ù…Ø­Ù„ÙŠØ§Ù‹ (ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ± host Ø¥Ù„Ù‰ '0.0.0.0' Ù„Ù„ÙˆØµÙˆÙ„ Ù…Ù† Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ø­Ù„ÙŠØ©)
    app.run(debug=True, host='0.0.0.0', port=5000)
